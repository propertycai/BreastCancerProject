"""
metrics.py - è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
åŒ…å«å„ç§æ¨¡å‹è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—å‡½æ•°
"""

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score
import pandas as pd


class MetricsCalculator:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""

    def __init__(self):
        pass

    def calculate_binary_metrics(self, y_true, y_pred):
        """
        è®¡ç®—äºŒåˆ†ç±»é—®é¢˜çš„è¯„ä¼°æŒ‡æ ‡

        Parameters:
        - y_true: çœŸå®æ ‡ç­¾
        - y_pred: é¢„æµ‹æ ‡ç­¾

        Returns:
        - metrics_dict: åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        metrics = {}

        # åŸºç¡€æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)
        metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)
        metrics['f1_score'] = f1_score(y_true, y_pred, zero_division=0)

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        metrics['confusion_matrix'] = cm

        # ä»æ··æ·†çŸ©é˜µä¸­æå–è¯¦ç»†æŒ‡æ ‡
        tn, fp, fn, tp = cm.ravel()
        metrics['true_negative'] = tn
        metrics['false_positive'] = fp
        metrics['false_negative'] = fn
        metrics['true_positive'] = tp

        # ç‰¹å¼‚æ€§
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0

        return metrics

    def cross_validation_metrics(self, model, X, y, cv=10, scoring_metrics=None):
        """
        è®¡ç®—äº¤å‰éªŒè¯æŒ‡æ ‡

        Parameters:
        - model: æœºå™¨å­¦ä¹ æ¨¡å‹
        - X: ç‰¹å¾æ•°æ®
        - y: æ ‡ç­¾æ•°æ®
        - cv: äº¤å‰éªŒè¯æŠ˜æ•°
        - scoring_metrics: è¯„åˆ†æŒ‡æ ‡åˆ—è¡¨

        Returns:
        - cv_results: äº¤å‰éªŒè¯ç»“æœ
        """
        if scoring_metrics is None:
            scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']

        cv_results = {}

        for metric in scoring_metrics:
            scores = cross_val_score(model, X, y, cv=cv, scoring=metric, n_jobs=-1)
            cv_results[metric] = {
                'mean': np.mean(scores),
                'std': np.std(scores),
                'scores': scores
            }

        return cv_results

    def print_detailed_report(self, y_true, y_pred, dataset_name=""):
        """
        æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š

        Parameters:
        - y_true: çœŸå®æ ‡ç­¾
        - y_pred: é¢„æµ‹æ ‡ç­¾
        - dataset_name: æ•°æ®é›†åç§°
        """
        print(f"\n{'=' * 60}")
        print(f"ğŸ“Š {dataset_name} è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
        print(f"{'=' * 60}")

        # åŸºç¡€æŒ‡æ ‡
        metrics = self.calculate_binary_metrics(y_true, y_pred)

        print(f"å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.4f}")
        print(f"ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.4f}")
        print(f"å¬å›ç‡ (Recall):    {metrics['recall']:.4f}")
        print(f"F1åˆ†æ•°:            {metrics['f1_score']:.4f}")
        print(f"ç‰¹å¼‚æ€§ (Specificity): {metrics['specificity']:.4f}")

        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"         é¢„æµ‹é˜´æ€§    é¢„æµ‹é˜³æ€§")
        print(f"çœŸå®é˜´æ€§    {metrics['true_negative']:4d}        {metrics['false_positive']:4d}")
        print(f"çœŸå®é˜³æ€§    {metrics['false_negative']:4d}        {metrics['true_positive']:4d}")

        # åˆ†ç±»æŠ¥å‘Š
        print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(y_true, y_pred, target_names=['è‰¯æ€§', 'æ¶æ€§'])
        print(report)

    def compare_models(self, models_results, metric='accuracy'):
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½

        Parameters:
        - models_results: æ¨¡å‹ç»“æœå­—å…¸ {æ¨¡å‹å: æŒ‡æ ‡å­—å…¸}
        - metric: æ¯”è¾ƒçš„æŒ‡æ ‡

        Returns:
        - comparison_df: æ¯”è¾ƒç»“æœDataFrame
        """
        comparison_data = []

        for model_name, results in models_results.items():
            if metric in results:
                row = {
                    'Model': model_name,
                    f'{metric.capitalize()}': results[metric],
                    'Precision': results.get('precision', 0),
                    'Recall': results.get('recall', 0),
                    'F1_Score': results.get('f1_score', 0)
                }
                comparison_data.append(row)

        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values(by=f'{metric.capitalize()}', ascending=False)

        return comparison_df

    def calculate_improvement(self, baseline_metrics, improved_metrics):
        """
        è®¡ç®—æ€§èƒ½æ”¹è¿›

        Parameters:
        - baseline_metrics: åŸºçº¿æ¨¡å‹æŒ‡æ ‡
        - improved_metrics: æ”¹è¿›æ¨¡å‹æŒ‡æ ‡

        Returns:
        - improvement_dict: æ”¹è¿›ç™¾åˆ†æ¯”å­—å…¸
        """
        improvement = {}
        metrics_list = ['accuracy', 'precision', 'recall', 'f1_score']

        for metric in metrics_list:
            if metric in baseline_metrics and metric in improved_metrics:
                baseline_val = baseline_metrics[metric]
                improved_val = improved_metrics[metric]
                improvement_pct = ((improved_val - baseline_val) / baseline_val) * 100
                improvement[metric] = improvement_pct

        return improvement


# æµ‹è¯•å‡½æ•°
def test_metrics_calculator():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    calculator = MetricsCalculator()

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    y_true = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
    y_pred = np.array([0, 1, 0, 0, 0, 1, 1, 1, 0, 1])

    # è®¡ç®—æŒ‡æ ‡
    metrics = calculator.calculate_binary_metrics(y_true, y_pred)

    print("æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—:")
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
    print(f"å¬å›ç‡: {metrics['recall']:.4f}")
    print(f"F1åˆ†æ•°: {metrics['f1_score']:.4f}")

    # æ‰“å°è¯¦ç»†æŠ¥å‘Š
    calculator.print_detailed_report(y_true, y_pred, "æµ‹è¯•æ•°æ®")


if __name__ == "__main__":
    test_metrics_calculator()
